{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Algorithms: A Visit  \n",
    "## Why look at this problem?  \n",
    "When I was working for my former employer, a digital advertising company in Japan, there were from time to time some anomaly detection tasks assigned to me which I didn't want to spend much time on. Basically I just threw all the data into some model like [Gaussian Processes](https://bugra.github.io/work/notes/2014-05-11/robust-regression-and-outlier-detection-via-gaussian-processes/) and highlighted the data which are not in the confidence/prediction interval as the output. I did not think much about if I can explain the results but there are much more to be explained than I thought. Indeed, feature engineering is important when you are dealing with data, and there are many powerful models that can handle unbalanced data or outliers very well like gaussian processes or boosted trees. However in most cases, those \"outliers\" do not only concern us in a data engineering way, but also in business sense. Which, requires the analysts to do more research about this specific problem and provides insights about the data.  \n",
    "Recently I read about a Q&A post on [Zhihu (a Quora-like site in China)](https://www.zhihu.com/question/280696035), the question is \"What are the popular anomaly detection algorithms in data mining?\". I went through all the answers and found that there are lot of stuff in this field. So I decided to look at this problem by implementing different methods myself. Could be lot of fun :)  \n",
    "\n",
    "## Some tools/libraries I will be using  \n",
    "- JupyterLab\n",
    "- pandas / dask  \n",
    "\n",
    "## Useful posts and papers  \n",
    "- [Metrics, Techniques and Tools of Anomaly Detection: A Survey](https://www.cse.wustl.edu/~jain/cse567-17/ftp/mttad/index.html)\n",
    "- [数据挖掘中常见的「异常检测」算法有哪些？](https://www.zhihu.com/question/280696035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Credit Card Fraud Detection  \n",
    "This is a kaggle dataset, problem description goes [here](https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3#). Some notes:  \n",
    "- Binary classification problem\n",
    "- Use AUC as the performance metric\n",
    "- Highly unbalanced:  frauds account takes for 0.172% of all transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
      "492 284807\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Okay let's take a look at the data first.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../fraud/creditcard.csv', header=0)\n",
    "cols = list(df.columns)\n",
    "# Row numbers of fraud records\n",
    "fraud_idx = list(df.loc[df['Class']==1].index.values)\n",
    "\n",
    "print (cols)\n",
    "print (len(fraud_idx), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1　Unsupervised Approaches  \n",
    "First I would like to try some unsupervised learning algorithms (pure statistical methods or clustering) and compare with their labels. By the end of this part hope I can have a general understanding of things below:  \n",
    "- How to implement unsupervised learning algorithms on anomaly detection  \n",
    "- What the data is like in specific problem and why some algorithms perfrom well  \n",
    "- Prons and cons of each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare our data first\n",
    "'''\n",
    "\n",
    "df_1 = df[cols[:-1]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1　KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134701 150106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Seems we need to normalize 'Time' and 'Amount'\n",
    "\n",
    "time_col = list(df_1['Time'])\n",
    "time_max = max(time_col)\n",
    "time_col = [x/time_max for x in time_col]\n",
    "\n",
    "amount_col = list(df_1['Amount'])\n",
    "amount_max = max(amount_col)\n",
    "amount_col = [x/amount_max for x in amount_col]\n",
    "\n",
    "df_1['Time'] = time_col\n",
    "df_1['Amount'] = amount_col\n",
    "\n",
    "X = df_1.as_matrix()\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "labels = list(kmeans.labels_)\n",
    "\n",
    "print (labels.count(0), labels.count(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
